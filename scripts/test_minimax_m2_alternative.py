"""
MiniMax-M2 å¤‡é€‰æµ‹è¯•è„šæœ¬ï¼ˆç®€åŒ–ç‰ˆï¼‰

å¦‚æœä¸»è„šæœ¬é‡åˆ°é…ç½®é—®é¢˜ï¼Œå¯ä»¥å°è¯•è¿™ä¸ªç®€åŒ–ç‰ˆæœ¬
"""

import os
import sys

# å¼ºåˆ¶ä½¿ç”¨CPUæ¨¡å¼
os.environ["CUDA_VISIBLE_DEVICES"] = ""
os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from datetime import datetime

# Add parent directory to path for imports
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# Import analyzer modules
from src.moe_analyzer import MoEAnalyzer
from src.visualizer import MoEVisualizer


def main():
    """MiniMax-M2 ç®€åŒ–æµ‹è¯•å‡½æ•°"""

    # é…ç½®å‚æ•°
    MODEL_PATH = "/hc550x10rz2-01/llms/MiniMax/MiniMax-M2"
    PROMPT = "è¯·å¸®æˆ‘å†™ä¸€ä¸ªpythonæ¸²æŸ“çš„ASCIIå­—ç¬¦é›†MandelbortSet"
    MAX_LENGTH = 512
    OUTPUT_DIR = f"./minimax_m2_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    PERIODIC_INTERVALS = [12, 24, 36]

    print("\n" + "=" * 70)
    print("MiniMax-M2 MoE Expert Activation Analysis (Alternative)")
    print("=" * 70)
    print(f"\nğŸ“ Model Path: {MODEL_PATH}")
    print(f"ğŸ“ Prompt: {PROMPT}")
    print(f"ğŸ–¥ï¸  Device: CPU")
    print(f"ğŸ”¢ Dtype: float32")
    print(f"\nâš ï¸  CPU Mode - CUDA disabled")
    print(f"    CUDA available: {torch.cuda.is_available()}")

    # åŠ è½½tokenizer
    print("\n[1/5] Loading Tokenizer...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
        print("âœ… Tokenizer loaded")
    except Exception as e:
        print(f"âŒ Error: {e}")
        return

    # åŠ è½½æ¨¡å‹ - å°è¯•æœ€ç®€å•çš„æ–¹å¼
    print("\n[2/5] Loading Model (this will take several minutes)...")
    print("    Using simplified loading strategy...")
    
    try:
        # æ–¹æ³•1ï¼šæœ€ç®€å•çš„åŠ è½½ï¼ˆæ¨èï¼‰
        print("    Attempting simple load with explicit dtype...")
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            dtype=torch.float32,
            device_map="cpu",
            low_cpu_mem_usage=True,
            trust_remote_code=True,
        )
        print("âœ… Model loaded successfully")
        
    except Exception as e1:
        print(f"    Failed: {e1}")
        print("\n    Trying alternative method...")
        
        try:
            # æ–¹æ³•2ï¼šä¸æŒ‡å®šdtypeï¼Œè®©å®ƒè‡ªåŠ¨é€‰æ‹©
            model = AutoModelForCausalLM.from_pretrained(
                MODEL_PATH,
                device_map="cpu",
                low_cpu_mem_usage=True,
                trust_remote_code=True,
            )
            # ç„¶åè½¬æ¢åˆ°float32
            model = model.float()
            print("âœ… Model loaded with auto dtype and converted to float32")
            
        except Exception as e2:
            print(f"    Failed: {e2}")
            print("\nâŒ Unable to load model. Please check:")
            print("    1. Model path is correct")
            print("    2. Model files are not corrupted")
            print("    3. You have sufficient memory (need ~200GB+)")
            return

    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    device = next(model.parameters()).device
    dtype = next(model.parameters()).dtype
    print(f"\nğŸ“Œ Model Device: {device}")
    print(f"ğŸ“Œ Model Dtype: {dtype}")

    # ç»Ÿè®¡MoEå±‚
    moe_count = sum(
        1 for layer in model.model.layers if hasattr(layer, "block_sparse_moe")
    )
    print(f"ğŸ“Œ Total Layers: {len(model.model.layers)}")
    print(f"ğŸ“Œ MoE Layers: {moe_count}")

    # åˆå§‹åŒ–åˆ†æå™¨
    print("\n[3/5] Initializing Analyzer...")
    analyzer = MoEAnalyzer(model, model_type="minimax")
    print(f"âœ… Analyzer ready")

    # å‡†å¤‡è¾“å…¥
    print("\n[4/5] Generating Text...")
    inputs = tokenizer(PROMPT, return_tensors="pt")
    inputs = {k: v.to("cpu") for k, v in inputs.items()}
    print(f"    Input tokens: {inputs['input_ids'].shape[1]}")

    # ç”Ÿæˆ
    print(f"    Generating up to {MAX_LENGTH} tokens (this will take a while)...")
    try:
        with analyzer.record():
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_length=MAX_LENGTH,
                    temperature=0.7,
                    do_sample=True,
                    top_p=0.9,
                    pad_token_id=tokenizer.eos_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                )

        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"\nâœ… Generated {outputs.shape[1]} tokens")
        print("\n" + "=" * 70)
        print("Generated Text:")
        print("=" * 70)
        print(generated_text)
        print("=" * 70)

    except Exception as e:
        print(f"\nâŒ Generation error: {e}")
        return

    # ç”Ÿæˆåˆ†ææŠ¥å‘Š
    print("\n[5/5] Generating Analysis Report...")
    summary = analyzer.get_summary_statistics()
    print(f"âœ… Analyzed {summary['num_moe_layers']} MoE layers")
    print(f"    Total tokens: {summary['total_tokens_analyzed']}")

    visualizer = MoEVisualizer()
    try:
        visualizer.create_comprehensive_report(
            analyzer, output_dir=OUTPUT_DIR, periodic_intervals=PERIODIC_INTERVALS
        )
        print(f"\nğŸ‰ Analysis complete! Results in: {OUTPUT_DIR}")
    except Exception as e:
        print(f"\nâŒ Visualization error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    print(
        """
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘      MiniMax-M2 MoE Analysis (Alternative/Simplified Version)      â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """
    )

    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Interrupted by user")
    except Exception as e:
        print(f"\n\nâŒ Unexpected error: {e}")
        import traceback
        traceback.print_exc()

