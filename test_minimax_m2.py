"""
MiniMax-M2 ä¸“ç”¨æµ‹è¯•è„šæœ¬

ç”¨äºæµ‹è¯•MiniMax-M2æ¨¡å‹çš„MoEä¸“å®¶æ¿€æ´»åˆ†æ
"""

import os
import argparse

# âš ï¸ é‡è¦ï¼šå¿…é¡»åœ¨å¯¼å…¥torchä¹‹å‰è®¾ç½®ç¯å¢ƒå˜é‡
# å¼ºåˆ¶ä½¿ç”¨CPUæ¨¡å¼ï¼Œç¦ç”¨CUDA
USE_CPU_MODE = True  # è®¾ç½®ä¸ºFalseä»¥ä½¿ç”¨GPU

if USE_CPU_MODE:
    os.environ["CUDA_VISIBLE_DEVICES"] = ""
    os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from datetime import datetime

# Import analyzer modules
from moe_analyzer import MoEAnalyzer
from visualizer import MoEVisualizer


def parse_arguments():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="MiniMax-M2 MoE Expert Activation Analysis",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Basic usage with default prompt
  python test_minimax_m2.py
  
  # Specify custom prompt
  python test_minimax_m2.py --prompt "Write a Python function to calculate fibonacci"
  
  # Load prompt from file
  python test_minimax_m2.py --prompt prompt.txt --max_tokens 1024
  
  # Enable expert weight similarity analysis
  python test_minimax_m2.py --enable_expert_similarity --n_jobs 64
  
  # Cache the float32 converted model for faster future runs
  python test_minimax_m2.py --cache_dir ./model_cache --dump_cache
  
  # Use cached model (skip conversion)
  python test_minimax_m2.py --cache_dir ./model_cache
  
  # Only convert and cache, don't run generation
  python test_minimax_m2.py --cache_dir ./model_cache --dump_only
        """,
    )

    # Prompt configuration
    parser.add_argument(
        "--prompt",
        type=str,
        default=None,
        help="Input prompt for generation. Can be a text string or path to a text file.",
    )
    parser.add_argument(
        "--max_tokens",
        type=int,
        default=None,
        help="Maximum number of NEW tokens to generate (default: 512)",
    )
    parser.add_argument(
        "--temperature",
        type=float,
        default=0.7,
        help="Sampling temperature (default: 0.7)",
    )
    parser.add_argument(
        "--top_p",
        type=float,
        default=0.9,
        help="Top-p sampling parameter (default: 0.9)",
    )
    parser.add_argument(
        "--no_sample",
        action="store_true",
        help="Use greedy decoding instead of sampling",
    )

    # Analysis configuration
    parser.add_argument(
        "--enable_expert_similarity",
        action="store_true",
        help="Enable expert weight similarity computation (time-consuming)",
    )
    parser.add_argument(
        "--n_jobs",
        type=int,
        default=None,
        help="Number of parallel jobs for expert similarity (default: auto)",
    )
    parser.add_argument(
        "--disable_structured_output",
        action="store_true",
        help="Disable structured data output (JSON)",
    )
    parser.add_argument(
        "--output_format",
        type=str,
        choices=["json", "jsonl", "pickle"],
        default="json",
        help="Structured output format (default: json)",
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default=None,
        help="Output directory (default: auto-generated with timestamp)",
    )

    # Model cache configuration
    parser.add_argument(
        "--cache_dir",
        type=str,
        default=None,
        help="Directory to save/load cached float32 model (for CPU mode)",
    )
    parser.add_argument(
        "--dump_cache",
        action="store_true",
        help="Save the converted float32 model to cache_dir after conversion",
    )
    parser.add_argument(
        "--dump_only",
        action="store_true",
        help="Only convert and cache the model, then exit (no generation)",
    )

    return parser.parse_args()


def load_prompt(prompt_arg):
    """
    Load prompt from argument or file.

    Args:
        prompt_arg: Prompt string or file path

    Returns:
        Prompt text
    """
    if prompt_arg is None:
        return None

    # Check if it's a file path
    if os.path.isfile(prompt_arg):
        print(f"ğŸ“„ Loading prompt from file: {prompt_arg}")
        try:
            with open(prompt_arg, "r", encoding="utf-8") as f:
                prompt_text = f.read().strip()
            print(f"âœ“ Loaded {len(prompt_text)} characters from file")
            return prompt_text
        except Exception as e:
            print(f"âš ï¸  Error reading file: {e}")
            print(f"   Using argument as prompt text instead")
            return prompt_arg
    else:
        # It's a direct prompt string
        return prompt_arg


def main():
    """MiniMax-M2 ä¸“ç”¨æµ‹è¯•å‡½æ•°"""

    # Parse command line arguments
    args = parse_arguments()

    # ==================== é…ç½®å‚æ•° ====================
    MODEL_PATH = "/hc550x10rz2-01/llms/MiniMax/MiniMax-M2"
    MODEL_TYPE = "minimax"

    # Prompt configuration (from args or default)
    DEFAULT_PROMPT = "Please help me write a Python program to render an ASCII character set of the Mandelbrot set"
    PROMPT = load_prompt(args.prompt) if args.prompt else DEFAULT_PROMPT

    # Generation configuration (from args or default)
    MAX_LENGTH = args.max_tokens if args.max_tokens else 512
    TEMPERATURE = args.temperature
    TOP_P = args.top_p
    DO_SAMPLE = not args.no_sample

    # Output directory (from args or auto-generated)
    if args.output_dir:
        OUTPUT_DIR = args.output_dir
    else:
        OUTPUT_DIR = f"./minimax_m2_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

    PERIODIC_INTERVALS = [12, 24, 36]  # æ£€æµ‹Î”=12, 24, 36çš„å‘¨æœŸæ€§æ¨¡å¼

    # CPUè¿è¡Œé…ç½®ï¼ˆä»æ–‡ä»¶å¼€å¤´çš„USE_CPU_MODEå˜é‡è¯»å–ï¼‰
    USE_CPU = USE_CPU_MODE
    DEVICE = "cpu" if USE_CPU else "auto"
    DTYPE = torch.float32 if USE_CPU else torch.bfloat16  # CPUä½¿ç”¨float32

    # ==================== åˆ†æé…ç½®å‚æ•° ====================
    # Expert Weight Similarity è®¡ç®—é…ç½® (from args or default)
    ENABLE_EXPERT_WEIGHT_SIMILARITY = args.enable_expert_similarity
    EXPERT_SIMILARITY_N_JOBS = args.n_jobs

    # ç»“æ„åŒ–æ•°æ®è¾“å‡ºé…ç½® (from args or default)
    ENABLE_STRUCTURED_OUTPUT = not args.disable_structured_output
    STRUCTURED_OUTPUT_FORMAT = args.output_format

    # Model cache configuration (from args)
    CACHE_DIR = args.cache_dir
    DUMP_CACHE = args.dump_cache or args.dump_only
    DUMP_ONLY = args.dump_only

    # å‘åå…¼å®¹ï¼ˆä¿ç•™æ—§å˜é‡åï¼‰
    SKIP_EXPERT_WEIGHT_SIMILARITY = not ENABLE_EXPERT_WEIGHT_SIMILARITY

    print("\n" + "=" * 70)
    print("MiniMax-M2 MoE Expert Activation Analysis")
    print("=" * 70)
    print(f"\nğŸ“ Model Path: {MODEL_PATH}")

    # Display prompt (truncate if too long)
    if len(PROMPT) > 100:
        prompt_display = PROMPT[:97] + "..."
        print(f"ğŸ“ Prompt: {prompt_display}")
        print(f"   (Full length: {len(PROMPT)} characters)")
    else:
        print(f"ğŸ“ Prompt: {PROMPT}")

    print(f"ğŸ“Š Max New Tokens: {MAX_LENGTH}")
    print(f"ğŸŒ¡ï¸  Temperature: {TEMPERATURE}")
    print(f"ğŸ² Top-p: {TOP_P}")
    print(f"ğŸ¯ Sampling: {'Enabled' if DO_SAMPLE else 'Disabled (Greedy)'}")
    print(f"ğŸ” Periodic Intervals: {PERIODIC_INTERVALS}")
    print(f"ğŸ’¾ Output Directory: {OUTPUT_DIR}")
    print(f"ğŸ–¥ï¸  Device: {DEVICE}")
    print(f"ğŸ”¢ Dtype: {DTYPE}")
    print(
        f"ğŸ“Š Expert Weight Similarity: {'Enabled' if ENABLE_EXPERT_WEIGHT_SIMILARITY else 'Disabled'}"
    )
    if ENABLE_EXPERT_WEIGHT_SIMILARITY:
        if EXPERT_SIMILARITY_N_JOBS is None:
            print(f"    Parallel jobs: Auto (all CPU cores, max 32)")
        else:
            print(f"    Parallel jobs: {EXPERT_SIMILARITY_N_JOBS}")
    print(
        f"ğŸ“„ Structured Output: {'Enabled' if ENABLE_STRUCTURED_OUTPUT else 'Disabled'} (format: {STRUCTURED_OUTPUT_FORMAT})"
    )

    # Cache configuration
    if CACHE_DIR:
        print(f"ğŸ’¾ Model Cache: {CACHE_DIR}")
        if DUMP_ONLY:
            print("    Mode: Dump-only (convert and save, then exit)")
        elif DUMP_CACHE:
            print("    Mode: Run analysis and save cache")
        else:
            print("    Mode: Load from cache (if exists)")
    else:
        print("ğŸ’¾ Model Cache: Disabled (will convert every time)")

    if USE_CPU:
        print("\nâš ï¸  CPU Mode Enabled (CUDA disabled via environment variable)")
        print("    Note: CPU inference will be significantly slower than GPU.")
        print("    For large models, this may take considerable time.")
        print(f"    CUDA available: {torch.cuda.is_available()}")
        print(f"    CUDA device count: {torch.cuda.device_count()}")

    # ==================== åŠ è½½æ¨¡å‹ ====================
    print("\n" + "-" * 70)
    print("[1/5] Loading Model and Tokenizer...")
    print("-" * 70)

    try:
        print("\nğŸ”„ Loading tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
        print("âœ… Tokenizer loaded successfully")

        # Check if we should load from cache
        load_from_cache = False
        if USE_CPU and CACHE_DIR and os.path.exists(CACHE_DIR):
            # Check for essential files
            # For large models, check for sharded format (model.safetensors.index.json)
            # For small models, check for single file (model.safetensors)
            config_exists = os.path.exists(os.path.join(CACHE_DIR, "config.json"))
            single_model_exists = os.path.exists(
                os.path.join(CACHE_DIR, "model.safetensors")
            )
            sharded_model_exists = os.path.exists(
                os.path.join(CACHE_DIR, "model.safetensors.index.json")
            )

            if config_exists and (single_model_exists or sharded_model_exists):
                load_from_cache = True
                if sharded_model_exists:
                    print(f"\nâœ¨ Found cached float32 model at: {CACHE_DIR}")
                    print("   (Sharded format detected)")
                else:
                    print(f"\nâœ¨ Found cached float32 model at: {CACHE_DIR}")
                print("   Loading from cache (skipping FP8â†’float32 conversion)...")
            else:
                print(f"\nâš ï¸  Cache directory exists but incomplete: {CACHE_DIR}")
                if not config_exists:
                    print("   Missing: config.json")
                if not single_model_exists and not sharded_model_exists:
                    print(
                        "   Missing: model.safetensors or model.safetensors.index.json"
                    )
                print("   Will perform conversion and save to cache.")

        if load_from_cache:
            # Load directly from cache
            print("\nğŸ”„ Loading cached float32 model...")
            model = AutoModelForCausalLM.from_pretrained(
                CACHE_DIR,
                dtype=DTYPE,
                device_map={"": "cpu"},
                low_cpu_mem_usage=True,
                trust_remote_code=True,
            )
            print("âœ… Cached model loaded successfully")
            print("   âš¡ Skipped FP8â†’float32 conversion (using cached version)")
        elif USE_CPU:
            # CPUæ¨¡å¼ï¼šç›´æ¥åŠ è½½åˆ°CPUï¼Œä½¿ç”¨float32
            print("    Loading configuration...")
            from transformers import AutoConfig

            # åŠ è½½é…ç½®ä½†ä¸ä½¿ç”¨é‡åŒ–
            config = AutoConfig.from_pretrained(MODEL_PATH, trust_remote_code=True)

            # åˆ é™¤é‡åŒ–é…ç½®å±æ€§ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if hasattr(config, "quantization_config"):
                delattr(config, "quantization_config")

            print("    Configuration loaded (quantization disabled)")
            print("    Loading model weights...")

            # ä½¿ç”¨ä¿®æ”¹åçš„é…ç½®åŠ è½½æ¨¡å‹
            model = AutoModelForCausalLM.from_pretrained(
                MODEL_PATH,
                config=config,
                dtype=DTYPE,  # ä½¿ç”¨dtypeè€Œétorch_dtype
                device_map={"": "cpu"},  # å¼ºåˆ¶ä½¿ç”¨CPU
                low_cpu_mem_usage=True,  # å‡å°‘CPUå†…å­˜ä½¿ç”¨
                trust_remote_code=True,
            )
        else:
            # GPUæ¨¡å¼ï¼šä½¿ç”¨device_mapè‡ªåŠ¨åˆ†é…
            model = AutoModelForCausalLM.from_pretrained(
                MODEL_PATH,
                dtype=DTYPE,  # ä½¿ç”¨dtypeè€Œétorch_dtype
                device_map=DEVICE,
                trust_remote_code=True,
            )

        print("âœ… Model loaded successfully")

        # å¼ºåˆ¶è½¬æ¢æ‰€æœ‰å‚æ•°å’Œç¼“å†²åŒºåˆ°float32ï¼ˆè§£å†³FP8æ··åˆé—®é¢˜ï¼‰
        if USE_CPU and not load_from_cache:
            print("\nğŸ”„ Converting all weights to float32...")
            print("    âš ï¸  Warning: Converting FP8 quantized model to float32")
            print("    This may cause some quality degradation in generation")
            model = model.float()  # è½¬æ¢æ‰€æœ‰å‚æ•°

            # ç¡®ä¿æ‰€æœ‰ç¼“å†²åŒºä¹Ÿæ˜¯float32
            for name, buffer in model.named_buffers():
                if buffer.dtype != torch.float32:
                    buffer.data = buffer.data.float()

            print("âœ… All weights converted to float32")
            print(
                "    Note: For production use, consider using a GPU-compatible environment"
            )

            # Save to cache if requested
            if DUMP_CACHE and CACHE_DIR:
                print(f"\nğŸ’¾ Saving converted model to cache: {CACHE_DIR}")
                print(
                    "    This will take a few minutes but will save time on future runs..."
                )
                try:
                    # Create cache directory if it doesn't exist
                    os.makedirs(CACHE_DIR, exist_ok=True)

                    # Save the model
                    model.save_pretrained(
                        CACHE_DIR,
                        safe_serialization=True,  # Use safetensors format
                        max_shard_size="5GB",  # Shard if model is too large
                    )

                    # Also save the tokenizer for convenience
                    tokenizer.save_pretrained(CACHE_DIR)

                    print(f"âœ… Model cached successfully to: {CACHE_DIR}")
                    print("   Next time, use --cache_dir to load instantly!")

                    # If dump_only mode, exit after saving
                    if DUMP_ONLY:
                        print("\n" + "=" * 70)
                        print(
                            "ğŸ‰ Dump-only mode: Model conversion and caching completed!"
                        )
                        print("=" * 70)
                        print(f"\nğŸ“ Cached model location: {CACHE_DIR}")
                        print("\nğŸ’¡ To use the cached model in future runs:")
                        print(f"   python test_minimax_m2.py --cache_dir {CACHE_DIR}")
                        print("\nâœ… Exiting (no generation performed)")
                        return

                except Exception as e:
                    print(f"âš ï¸  Warning: Failed to save cache: {e}")
                    print("   Continuing with analysis...")

        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        device = next(model.parameters()).device
        dtype = next(model.parameters()).dtype
        print(f"\nğŸ“Œ Model Device: {device}")
        print(f"ğŸ“Œ Model Dtype: {dtype}")

        # ç»Ÿè®¡MoEå±‚æ•°é‡
        moe_layer_count = 0
        total_layers = len(model.model.layers)
        for layer in model.model.layers:
            if hasattr(layer, "block_sparse_moe"):
                moe_layer_count += 1
        print(f"ğŸ“Œ Total Layers: {total_layers}")
        print(f"ğŸ“Œ MoE Layers: {moe_layer_count}")

    except Exception as e:
        print(f"\nâŒ Error loading model: {e}")
        print("\nğŸ’¡ Tips:")
        print("  - Check if the model path is correct")
        print("  - Make sure you have enough GPU memory")
        print("  - Try using torch_dtype=torch.float16 if bfloat16 is not supported")
        return

    # ==================== åˆå§‹åŒ–åˆ†æå™¨ ====================
    print("\n" + "-" * 70)
    print("[2/5] Initializing MoE Analyzer...")
    print("-" * 70)

    analyzer = MoEAnalyzer(model, model_type=MODEL_TYPE)
    print(f"âœ… Analyzer initialized")
    print(f"ğŸ“Œ Detected Model Type: {analyzer.model_type}")

    # ==================== å‡†å¤‡è¾“å…¥ ====================
    print("\n" + "-" * 70)
    print("[3/5] Preparing Input and Running Generation...")
    print("-" * 70)

    print(f"\nğŸ“ Tokenizing prompt...")
    inputs = tokenizer(PROMPT, return_tensors="pt")
    device = next(model.parameters()).device
    inputs = {k: v.to(device) for k, v in inputs.items()}

    input_length = inputs["input_ids"].shape[1]
    print(f"âœ… Input tokens: {input_length}")

    # ==================== è¿è¡Œç”Ÿæˆå¹¶è®°å½•æ¿€æ´» ====================
    print(f"\nğŸš€ Generating (max_new_tokens={MAX_LENGTH})...")
    print(
        f"   Input tokens: {input_length}, will generate up to {MAX_LENGTH} new tokens"
    )
    print("â±ï¸  This will take a while, please wait...")

    # Debug: Check tokenizer configuration
    print(f"\nğŸ” Tokenizer info:")
    print(f"   - eos_token_id: {tokenizer.eos_token_id}")
    print(f"   - pad_token_id: {tokenizer.pad_token_id}")
    print(f"   - bos_token_id: {tokenizer.bos_token_id}")

    try:
        with analyzer.record():
            with torch.no_grad():
                # Prepare generation kwargs
                gen_kwargs = {
                    **inputs,
                    "max_new_tokens": MAX_LENGTH,
                    "pad_token_id": (
                        tokenizer.pad_token_id
                        if tokenizer.pad_token_id is not None
                        else tokenizer.eos_token_id
                    ),
                }

                if DO_SAMPLE:
                    gen_kwargs.update(
                        {
                            "do_sample": True,
                            "temperature": TEMPERATURE,
                            "top_p": TOP_P,
                        }
                    )
                else:
                    gen_kwargs.update(
                        {
                            "do_sample": False,
                        }
                    )

                print(f"\nâš™ï¸  Generation parameters:")
                print(f"   - max_new_tokens: {MAX_LENGTH}")
                print(f"   - do_sample: {gen_kwargs['do_sample']}")
                if DO_SAMPLE:
                    print(f"   - temperature: {TEMPERATURE}")
                    print(f"   - top_p: {TOP_P}")
                print(f"   - pad_token_id: {gen_kwargs['pad_token_id']}")
                print()

                outputs = model.generate(**gen_kwargs)

        output_length = outputs.shape[1]
        generated_length = output_length - input_length

        print(f"\nâœ… Generation completed!")
        print(f"ğŸ“Œ Total tokens: {output_length}")
        print(f"ğŸ“Œ Generated tokens: {generated_length}")

        # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

        print("\n" + "=" * 70)
        print("Generated Text:")
        print("=" * 70)
        print(generated_text)
        print("=" * 70)

    except Exception as e:
        print(f"\nâŒ Error during generation: {e}")
        print("\nğŸ’¡ Tips:")
        print("  - Try reducing --max_tokens")
        print("  - Try using greedy decoding: --no_sample")
        print("  - Try lowering temperature: --temperature 0.3")
        print("  - Check memory usage")
        import traceback

        traceback.print_exc()
        return

    # ==================== è·å–ç»Ÿè®¡æ‘˜è¦ ====================
    print("\n" + "-" * 70)
    print("[4/5] Processing Collected Data...")
    print("-" * 70)

    summary = analyzer.get_summary_statistics()
    print(f"\nâœ… Data processing completed!")
    print(f"ğŸ“Œ MoE Layers Analyzed: {summary['num_moe_layers']}")
    print(f"ğŸ“Œ Layer Indices: {summary['layer_indices']}")
    print(f"ğŸ“Œ Total Tokens Analyzed: {summary['total_tokens_analyzed']}")

    if summary["num_experts_per_layer"]:
        first_layer = list(summary["num_experts_per_layer"].keys())[0]
        num_experts = summary["num_experts_per_layer"][first_layer]
        top_k = summary["top_k_per_layer"][first_layer]
        print(f"ğŸ“Œ Experts per Layer: {num_experts}")
        print(f"ğŸ“Œ Top-K per Token: {top_k}")

    # ==================== ç”Ÿæˆå¯è§†åŒ– ====================
    print("\n" + "-" * 70)
    print("[5/5] Generating Visualizations and Reports...")
    print("-" * 70)

    if SKIP_EXPERT_WEIGHT_SIMILARITY:
        print("\nâ­ï¸  Note: Expert weight similarity analysis is DISABLED")
        print("   To enable: Set ENABLE_EXPERT_WEIGHT_SIMILARITY = True in the script")
        print("   To control parallelism: Set EXPERT_SIMILARITY_N_JOBS = <number>")
        print("   (Recommended: Use parallel mode for faster computation)")

    visualizer = MoEVisualizer()

    try:
        if SKIP_EXPERT_WEIGHT_SIMILARITY:
            # æ‰‹åŠ¨è°ƒç”¨å„ä¸ªåˆ†ææ­¥éª¤ï¼Œè·³è¿‡expert weight similarity
            os.makedirs(OUTPUT_DIR, exist_ok=True)

            print("\n" + "=" * 60)
            print("Generating MoE Analysis Report (Fast Mode)")
            print("=" * 60)

            # 1. Expert Activation Heatmap (2D)
            print("\n[1/6] Generating expert activation heatmap (2D)...")
            activation_matrix = analyzer.get_expert_activation_matrix()
            visualizer.plot_expert_activation_heatmap(
                activation_matrix,
                layer_indices=analyzer.layer_indices,
                save_path=os.path.join(OUTPUT_DIR, "expert_activation_heatmap.html"),
            )

            # 2. Expert Activation 3D Visualization (NEW!)
            print("\n[2/6] Generating expert activation 3D visualization...")
            print("    This may take a moment for large models...")
            visualizer.plot_expert_activation_3d(
                analyzer,
                save_path=os.path.join(OUTPUT_DIR, "expert_activation_3d.html"),
                max_tokens=50,  # Limit for performance
                max_layers=10,  # Show up to 10 layers
                max_experts=64,  # Show up to 64 experts
            )

            # 3. Layer Correlation Matrix
            print("\n[3/6] Computing and plotting layer correlation matrix...")
            correlation_data = analyzer.compute_layer_correlation_matrix(delta_max=30)
            visualizer.plot_layer_correlation_matrix(
                correlation_data,
                save_path=os.path.join(OUTPUT_DIR, "layer_correlation_matrix.html"),
            )

            # 4. Periodic Patterns
            print(
                f"\n[4/6] Analyzing periodic patterns (intervals: {PERIODIC_INTERVALS})..."
            )
            periodic_data = analyzer.compute_periodic_patterns(
                intervals=PERIODIC_INTERVALS
            )
            visualizer.plot_periodic_pattern(
                periodic_data,
                save_path=os.path.join(OUTPUT_DIR, "periodic_pattern_analysis.html"),
            )

            # 5. Router Weight Similarity
            print("\n[5/6] Computing router weight similarity...")
            router_sim_data = analyzer.compute_router_weight_similarity()
            visualizer.plot_router_similarity_matrix(
                router_sim_data,
                save_path=os.path.join(OUTPUT_DIR, "router_similarity_matrix.html"),
            )

            # 6. Expert Weight Similarity (å¦‚æœå¯ç”¨)
            if ENABLE_EXPERT_WEIGHT_SIMILARITY:
                print(
                    f"\n[6/6] Computing expert weight similarity (delta={PERIODIC_INTERVALS[0]})..."
                )
                print(
                    f"    Using parallel mode with {EXPERT_SIMILARITY_N_JOBS or 'auto'} jobs"
                )
                try:
                    expert_sim_data = analyzer.compute_expert_weight_similarity(
                        delta=PERIODIC_INTERVALS[0],
                        use_parallel=True,
                        n_jobs=EXPERT_SIMILARITY_N_JOBS,
                    )
                    visualizer.plot_expert_weight_similarity(
                        expert_sim_data,
                        save_path=os.path.join(
                            OUTPUT_DIR, "expert_weight_similarity.html"
                        ),
                    )
                    print("    âœ“ Expert weight similarity computed successfully")
                except KeyboardInterrupt:
                    print("\n    â­ï¸  Expert weight similarity computation interrupted")
                except Exception as e:
                    print(f"\n    âš ï¸  Error in expert weight similarity: {e}")
            else:
                print(
                    "\n[6/6] Skipping expert weight similarity (ENABLE_EXPERT_WEIGHT_SIMILARITY=False)"
                )

            # Summary
            summary_final = analyzer.get_summary_statistics()
            visualizer._save_summary_report(summary_final, periodic_data, OUTPUT_DIR)

            # Export structured data (if enabled)
            if ENABLE_STRUCTURED_OUTPUT:
                print("\n" + "-" * 60)
                print("Exporting Structured Data")
                print("-" * 60)
                visualizer.export_structured_data(
                    analyzer,
                    output_dir=OUTPUT_DIR,
                    format=STRUCTURED_OUTPUT_FORMAT,
                    include_raw_data=False,  # Set to True to include raw routing probabilities
                )

            print("\n" + "=" * 60)
            print("Fast Mode Analysis complete!")
            print("=" * 60)
        else:
            visualizer.create_comprehensive_report(
                analyzer, output_dir=OUTPUT_DIR, periodic_intervals=PERIODIC_INTERVALS
            )

        print("\n" + "=" * 70)
        print("âœ… Analysis Complete!")
        print("=" * 70)

        print(f"\nğŸ“‚ Results saved to: {OUTPUT_DIR}")
        print("\nğŸ“Š Generated files:")

        files = [
            ("expert_activation_heatmap.html", "ä¸“å®¶æ¿€æ´»çƒ­åº¦å›¾ (2D)"),
            ("expert_activation_3d.html", "ä¸“å®¶æ¿€æ´»3Då¯è§†åŒ– (äº¤äº’å¼)"),
            ("layer_correlation_matrix.html", "å±‚é—´ç›¸å…³æ€§çŸ©é˜µ"),
            ("periodic_pattern_analysis.html", "å‘¨æœŸæ€§æ¨¡å¼åˆ†æ"),
            ("periodic_pattern_analysis_detailed.html", "å‘¨æœŸæ€§æ¨¡å¼è¯¦ç»†åˆ†æ"),
            ("router_similarity_matrix.html", "è·¯ç”±å™¨ç›¸ä¼¼åº¦çŸ©é˜µ"),
            ("router_similarity_matrix_column_norms.html", "è·¯ç”±å™¨åˆ—èŒƒæ•°ç›¸å…³æ€§"),
            ("expert_weight_similarity.html", "ä¸“å®¶æƒé‡ç›¸ä¼¼åº¦åˆ†å¸ƒ"),
            ("expert_weight_similarity_scatter.html", "ä¸“å®¶æƒé‡ç›¸ä¼¼åº¦æ•£ç‚¹å›¾"),
            ("summary_report.txt", "ç»Ÿè®¡æ‘˜è¦æŠ¥å‘Š"),
            (
                ("analysis_data.json", "ç»“æ„åŒ–æ•°æ®æŠ¥å‘Š (JSON)")
                if ENABLE_STRUCTURED_OUTPUT
                else None
            ),
            (
                ("analysis_summary.json", "åˆ†ææ‘˜è¦ (JSON)")
                if ENABLE_STRUCTURED_OUTPUT
                else None
            ),
        ]

        files = [f for f in files if f is not None]  # è¿‡æ»¤None

        for filename, description in files:
            filepath = os.path.join(OUTPUT_DIR, filename)
            if os.path.exists(filepath):
                size_kb = os.path.getsize(filepath) / 1024
                print(f"  âœ… {filename:<45} ({size_kb:.1f} KB) - {description}")

        print("\n" + "=" * 70)
        print("ğŸ‰ All Done!")
        print("=" * 70)
        print("\nğŸ’¡ Next Steps:")
        print(f"  1. Open the HTML files in your browser to explore the visualizations")
        print(
            f"  2. Check {os.path.join(OUTPUT_DIR, 'summary_report.txt')} for statistics"
        )
        print(f"  3. Look for bright bands at Î”=12 in the correlation matrix!")

        print("\nğŸ” Key Things to Look For:")
        print("  â€¢ Are there periodic patterns at Î”=12, 24, or 36?")
        print("  â€¢ Which experts are most frequently activated?")
        print(
            "  â€¢ Is there high similarity between routers at specific layer distances?"
        )
        print(
            "  â€¢ Do expert weights show patterns suggesting layer stacking/upcycling?"
        )

    except Exception as e:
        print(f"\nâŒ Error generating visualizations: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    print(
        """
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘         MiniMax-M2 MoE Expert Activation Analysis Tool             â•‘
    â•‘                                                                    â•‘
    â•‘  This script analyzes expert activation patterns in MiniMax-M2    â•‘
    â•‘  and generates interactive visualizations to explore:              â•‘
    â•‘  â€¢ Expert activation probabilities                                 â•‘
    â•‘  â€¢ Layer-to-layer correlations                                     â•‘
    â•‘  â€¢ Periodic patterns (Î”=12, 24, 36...)                           â•‘
    â•‘  â€¢ Router and expert weight similarities                           â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """
    )

    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Interrupted by user")
    except Exception as e:
        print(f"\n\nâŒ Unexpected error: {e}")
        import traceback

        traceback.print_exc()
