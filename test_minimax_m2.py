"""
MiniMax-M2 ä¸“ç”¨æµ‹è¯•è„šæœ¬

ç”¨äºæµ‹è¯•MiniMax-M2æ¨¡å‹çš„MoEä¸“å®¶æ¿€æ´»åˆ†æ
"""

import os

# âš ï¸ é‡è¦ï¼šå¿…é¡»åœ¨å¯¼å…¥torchä¹‹å‰è®¾ç½®ç¯å¢ƒå˜é‡
# å¼ºåˆ¶ä½¿ç”¨CPUæ¨¡å¼ï¼Œç¦ç”¨CUDA
USE_CPU_MODE = True  # è®¾ç½®ä¸ºFalseä»¥ä½¿ç”¨GPU

if USE_CPU_MODE:
    os.environ["CUDA_VISIBLE_DEVICES"] = ""
    os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from datetime import datetime

# Import analyzer modules
from moe_analyzer import MoEAnalyzer
from visualizer import MoEVisualizer


def main():
    """MiniMax-M2 ä¸“ç”¨æµ‹è¯•å‡½æ•°"""

    # ==================== é…ç½®å‚æ•° ====================
    MODEL_PATH = "/hc550x10rz2-01/llms/MiniMax/MiniMax-M2"
    MODEL_TYPE = "minimax"
    PROMPT = "Please help me write a Python program to render an ASCII character set of the Mandelbrot set"
    MAX_LENGTH = 512
    OUTPUT_DIR = f"./minimax_m2_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    PERIODIC_INTERVALS = [12, 24, 36]  # æ£€æµ‹Î”=12, 24, 36çš„å‘¨æœŸæ€§æ¨¡å¼

    # CPUè¿è¡Œé…ç½®ï¼ˆä»æ–‡ä»¶å¼€å¤´çš„USE_CPU_MODEå˜é‡è¯»å–ï¼‰
    USE_CPU = USE_CPU_MODE
    DEVICE = "cpu" if USE_CPU else "auto"
    DTYPE = torch.float32 if USE_CPU else torch.bfloat16  # CPUä½¿ç”¨float32

    print("\n" + "=" * 70)
    print("MiniMax-M2 MoE Expert Activation Analysis")
    print("=" * 70)
    print(f"\nğŸ“ Model Path: {MODEL_PATH}")
    print(f"ğŸ“ Prompt: {PROMPT}")
    print(f"ğŸ“Š Max Length: {MAX_LENGTH}")
    print(f"ğŸ” Periodic Intervals: {PERIODIC_INTERVALS}")
    print(f"ğŸ’¾ Output Directory: {OUTPUT_DIR}")
    print(f"ğŸ–¥ï¸  Device: {DEVICE}")
    print(f"ğŸ”¢ Dtype: {DTYPE}")

    if USE_CPU:
        print("\nâš ï¸  CPU Mode Enabled (CUDA disabled via environment variable)")
        print("    Note: CPU inference will be significantly slower than GPU.")
        print("    For large models, this may take considerable time.")
        print(f"    CUDA available: {torch.cuda.is_available()}")
        print(f"    CUDA device count: {torch.cuda.device_count()}")

    # ==================== åŠ è½½æ¨¡å‹ ====================
    print("\n" + "-" * 70)
    print("[1/5] Loading Model and Tokenizer...")
    print("-" * 70)

    try:
        print("\nğŸ”„ Loading tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
        print("âœ… Tokenizer loaded successfully")

        print("\nğŸ”„ Loading model (this may take a while)...")

        if USE_CPU:
            # CPUæ¨¡å¼ï¼šç›´æ¥åŠ è½½åˆ°CPUï¼Œä½¿ç”¨float32
            print("    Loading configuration...")
            from transformers import AutoConfig

            # åŠ è½½é…ç½®ä½†ä¸ä½¿ç”¨é‡åŒ–
            config = AutoConfig.from_pretrained(MODEL_PATH, trust_remote_code=True)

            # åˆ é™¤é‡åŒ–é…ç½®å±æ€§ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if hasattr(config, "quantization_config"):
                delattr(config, "quantization_config")

            print("    Configuration loaded (quantization disabled)")
            print("    Loading model weights...")

            # ä½¿ç”¨ä¿®æ”¹åçš„é…ç½®åŠ è½½æ¨¡å‹
            model = AutoModelForCausalLM.from_pretrained(
                MODEL_PATH,
                config=config,
                dtype=DTYPE,  # ä½¿ç”¨dtypeè€Œétorch_dtype
                device_map={"": "cpu"},  # å¼ºåˆ¶ä½¿ç”¨CPU
                low_cpu_mem_usage=True,  # å‡å°‘CPUå†…å­˜ä½¿ç”¨
                trust_remote_code=True,
            )
        else:
            # GPUæ¨¡å¼ï¼šä½¿ç”¨device_mapè‡ªåŠ¨åˆ†é…
            model = AutoModelForCausalLM.from_pretrained(
                MODEL_PATH,
                dtype=DTYPE,  # ä½¿ç”¨dtypeè€Œétorch_dtype
                device_map=DEVICE,
                trust_remote_code=True,
            )

        print("âœ… Model loaded successfully")

        # å¼ºåˆ¶è½¬æ¢æ‰€æœ‰å‚æ•°å’Œç¼“å†²åŒºåˆ°float32ï¼ˆè§£å†³FP8æ··åˆé—®é¢˜ï¼‰
        if USE_CPU:
            print("\nğŸ”„ Converting all weights to float32...")
            model = model.float()  # è½¬æ¢æ‰€æœ‰å‚æ•°

            # ç¡®ä¿æ‰€æœ‰ç¼“å†²åŒºä¹Ÿæ˜¯float32
            for name, buffer in model.named_buffers():
                if buffer.dtype != torch.float32:
                    buffer.data = buffer.data.float()

            print("âœ… All weights converted to float32")

        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        device = next(model.parameters()).device
        dtype = next(model.parameters()).dtype
        print(f"\nğŸ“Œ Model Device: {device}")
        print(f"ğŸ“Œ Model Dtype: {dtype}")

        # ç»Ÿè®¡MoEå±‚æ•°é‡
        moe_layer_count = 0
        total_layers = len(model.model.layers)
        for layer in model.model.layers:
            if hasattr(layer, "block_sparse_moe"):
                moe_layer_count += 1
        print(f"ğŸ“Œ Total Layers: {total_layers}")
        print(f"ğŸ“Œ MoE Layers: {moe_layer_count}")

    except Exception as e:
        print(f"\nâŒ Error loading model: {e}")
        print("\nğŸ’¡ Tips:")
        print("  - Check if the model path is correct")
        print("  - Make sure you have enough GPU memory")
        print("  - Try using torch_dtype=torch.float16 if bfloat16 is not supported")
        return

    # ==================== åˆå§‹åŒ–åˆ†æå™¨ ====================
    print("\n" + "-" * 70)
    print("[2/5] Initializing MoE Analyzer...")
    print("-" * 70)

    analyzer = MoEAnalyzer(model, model_type=MODEL_TYPE)
    print(f"âœ… Analyzer initialized")
    print(f"ğŸ“Œ Detected Model Type: {analyzer.model_type}")

    # ==================== å‡†å¤‡è¾“å…¥ ====================
    print("\n" + "-" * 70)
    print("[3/5] Preparing Input and Running Generation...")
    print("-" * 70)

    print(f"\nğŸ“ Tokenizing prompt...")
    inputs = tokenizer(PROMPT, return_tensors="pt")
    device = next(model.parameters()).device
    inputs = {k: v.to(device) for k, v in inputs.items()}

    input_length = inputs["input_ids"].shape[1]
    print(f"âœ… Input tokens: {input_length}")

    # ==================== è¿è¡Œç”Ÿæˆå¹¶è®°å½•æ¿€æ´» ====================
    print(f"\nğŸš€ Generating (max_length={MAX_LENGTH})...")
    print("â±ï¸  This will take a while, please wait...")

    try:
        with analyzer.record():
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_length=MAX_LENGTH,
                    temperature=0.7,
                    do_sample=True,
                    top_p=0.9,
                    pad_token_id=tokenizer.eos_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                )

        output_length = outputs.shape[1]
        generated_length = output_length - input_length

        print(f"\nâœ… Generation completed!")
        print(f"ğŸ“Œ Total tokens: {output_length}")
        print(f"ğŸ“Œ Generated tokens: {generated_length}")

        # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

        print("\n" + "=" * 70)
        print("Generated Text:")
        print("=" * 70)
        print(generated_text)
        print("=" * 70)

    except Exception as e:
        print(f"\nâŒ Error during generation: {e}")
        print("\nğŸ’¡ Tips:")
        print("  - Try reducing max_length")
        print("  - Check GPU memory usage")
        return

    # ==================== è·å–ç»Ÿè®¡æ‘˜è¦ ====================
    print("\n" + "-" * 70)
    print("[4/5] Processing Collected Data...")
    print("-" * 70)

    summary = analyzer.get_summary_statistics()
    print(f"\nâœ… Data processing completed!")
    print(f"ğŸ“Œ MoE Layers Analyzed: {summary['num_moe_layers']}")
    print(f"ğŸ“Œ Layer Indices: {summary['layer_indices']}")
    print(f"ğŸ“Œ Total Tokens Analyzed: {summary['total_tokens_analyzed']}")

    if summary["num_experts_per_layer"]:
        first_layer = list(summary["num_experts_per_layer"].keys())[0]
        num_experts = summary["num_experts_per_layer"][first_layer]
        top_k = summary["top_k_per_layer"][first_layer]
        print(f"ğŸ“Œ Experts per Layer: {num_experts}")
        print(f"ğŸ“Œ Top-K per Token: {top_k}")

    # ==================== ç”Ÿæˆå¯è§†åŒ– ====================
    print("\n" + "-" * 70)
    print("[5/5] Generating Visualizations and Reports...")
    print("-" * 70)

    visualizer = MoEVisualizer()

    try:
        visualizer.create_comprehensive_report(
            analyzer, output_dir=OUTPUT_DIR, periodic_intervals=PERIODIC_INTERVALS
        )

        print("\n" + "=" * 70)
        print("âœ… Analysis Complete!")
        print("=" * 70)

        print(f"\nğŸ“‚ Results saved to: {OUTPUT_DIR}")
        print("\nğŸ“Š Generated files:")

        files = [
            ("expert_activation_heatmap.html", "ä¸“å®¶æ¿€æ´»çƒ­åº¦å›¾"),
            ("layer_correlation_matrix.html", "å±‚é—´ç›¸å…³æ€§çŸ©é˜µ"),
            ("periodic_pattern_analysis.html", "å‘¨æœŸæ€§æ¨¡å¼åˆ†æ"),
            ("periodic_pattern_analysis_detailed.html", "å‘¨æœŸæ€§æ¨¡å¼è¯¦ç»†åˆ†æ"),
            ("router_similarity_matrix.html", "è·¯ç”±å™¨ç›¸ä¼¼åº¦çŸ©é˜µ"),
            ("router_similarity_matrix_column_norms.html", "è·¯ç”±å™¨åˆ—èŒƒæ•°ç›¸å…³æ€§"),
            ("expert_weight_similarity.html", "ä¸“å®¶æƒé‡ç›¸ä¼¼åº¦åˆ†å¸ƒ"),
            ("expert_weight_similarity_scatter.html", "ä¸“å®¶æƒé‡ç›¸ä¼¼åº¦æ•£ç‚¹å›¾"),
            ("summary_report.txt", "ç»Ÿè®¡æ‘˜è¦æŠ¥å‘Š"),
        ]

        for filename, description in files:
            filepath = os.path.join(OUTPUT_DIR, filename)
            if os.path.exists(filepath):
                size_kb = os.path.getsize(filepath) / 1024
                print(f"  âœ… {filename:<45} ({size_kb:.1f} KB) - {description}")

        print("\n" + "=" * 70)
        print("ğŸ‰ All Done!")
        print("=" * 70)
        print("\nğŸ’¡ Next Steps:")
        print(f"  1. Open the HTML files in your browser to explore the visualizations")
        print(
            f"  2. Check {os.path.join(OUTPUT_DIR, 'summary_report.txt')} for statistics"
        )
        print(f"  3. Look for bright bands at Î”=12 in the correlation matrix!")

        print("\nğŸ” Key Things to Look For:")
        print("  â€¢ Are there periodic patterns at Î”=12, 24, or 36?")
        print("  â€¢ Which experts are most frequently activated?")
        print(
            "  â€¢ Is there high similarity between routers at specific layer distances?"
        )
        print(
            "  â€¢ Do expert weights show patterns suggesting layer stacking/upcycling?"
        )

    except Exception as e:
        print(f"\nâŒ Error generating visualizations: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    print(
        """
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘         MiniMax-M2 MoE Expert Activation Analysis Tool             â•‘
    â•‘                                                                    â•‘
    â•‘  This script analyzes expert activation patterns in MiniMax-M2    â•‘
    â•‘  and generates interactive visualizations to explore:              â•‘
    â•‘  â€¢ Expert activation probabilities                                 â•‘
    â•‘  â€¢ Layer-to-layer correlations                                     â•‘
    â•‘  â€¢ Periodic patterns (Î”=12, 24, 36...)                           â•‘
    â•‘  â€¢ Router and expert weight similarities                           â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """
    )

    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Interrupted by user")
    except Exception as e:
        print(f"\n\nâŒ Unexpected error: {e}")
        import traceback

        traceback.print_exc()
